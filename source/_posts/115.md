---
title: sklearn DBScan
date: 2017-12-21 15:44:52
tags: sklearn
categories: sklearn
---

K均值聚类使用非常广泛，作为最为古老的聚类方法，它的算法非常简单，而且速度很快。但是其缺点在于它不能识别非球形的簇。我们可以用一个简单的例子来观察K均值聚类的弱点。我们先构造一些人为数据，它是基于sin函数和cos函数构成的两组点。如果我们用传统的K均值聚类，结果如下图所示。其聚类结果是不理想的，因为它不能识别非球形的簇。

**好处** 

1. 与K-means方法相比，DBSCAN不需要事先知道要形成的簇类的数量。 
2. 与K-means方法相比，DBSCAN可以发现任意形状的簇类。 
3. 同时，DBSCAN能够识别出噪声点。 
4. 4.DBSCAN对于数据库中样本的顺序不敏感，即Pattern的输入顺序对结果的影响不大。但是，对于处于簇类之间边界样本，可能会根据哪个簇类优先被探测到而其归属有所摆动。 

**缺点** 

1. DBScan不能很好反映高维数据。 
2. DBScan不能很好反映数据集以变化的密度。

<!--more-->

## 自己写代码实现

#### 运行环境

- Pyhton3
- numpy(科学计算包)
- matplotlib(画图所需，不画图可不必)

#### 计算过程

```
st=>start: 开始
e=>end: 结束
op1=>operation: 读入数据
cond=>condition: 是否还有未分类数据
op2=>operation: 找一未分类点扩散
op3=>operation: 输出结果

st->op1->op2->cond
cond(yes)->op2
cond(no)->op3->e
```

#### 输入样例

```python
/* 788points.txt */
15.55,28.65
14.9,27.55
14.45,28.35
14.15,28.8
13.75,28.05
13.35,28.45
13,29.15
13.45,27.5
13.6,26.5
12.8,27.35
12.4,27.85
12.3,28.4
12.2,28.65
13.4,25.1
12.95,25.95
```

788points.txt完整文件：[下载](http://pan.baidu.com/s/1i4o7wxf)

#### 代码实现

```python
# -*- coding: utf-8 -*-
__author__ = 'Wsine'

import numpy as np
import matplotlib.pyplot as plt
import math
import time

UNCLASSIFIED = False
NOISE = 0

def loadDataSet(fileName, splitChar='\t'):
    """
    输入：文件名
    输出：数据集
    描述：从文件读入数据集
    """
    dataSet = []
    with open(fileName) as fr:
        for line in fr.readlines():
            curline = line.strip().split(splitChar)
            fltline = list(map(float, curline))
            dataSet.append(fltline)
    return dataSet

def dist(a, b):
    """
    输入：向量A, 向量B
    输出：两个向量的欧式距离
    """
    return math.sqrt(np.power(a - b, 2).sum())

def eps_neighbor(a, b, eps):
    """
    输入：向量A, 向量B
    输出：是否在eps范围内
    """
    return dist(a, b) < eps

def region_query(data, pointId, eps):
    """
    输入：数据集, 查询点id, 半径大小
    输出：在eps范围内的点的id
    """
    nPoints = data.shape[1]
    seeds = []
    for i in range(nPoints):
        if eps_neighbor(data[:, pointId], data[:, i], eps):
            seeds.append(i)
    return seeds

def expand_cluster(data, clusterResult, pointId, clusterId, eps, minPts):
    """
    输入：数据集, 分类结果, 待分类点id, 簇id, 半径大小, 最小点个数
    输出：能否成功分类
    """
    seeds = region_query(data, pointId, eps)
    if len(seeds) < minPts: # 不满足minPts条件的为噪声点
        clusterResult[pointId] = NOISE
        return False
    else:
        clusterResult[pointId] = clusterId # 划分到该簇
        for seedId in seeds:
            clusterResult[seedId] = clusterId

        while len(seeds) > 0: # 持续扩张
            currentPoint = seeds[0]
            queryResults = region_query(data, currentPoint, eps)
            if len(queryResults) >= minPts:
                for i in range(len(queryResults)):
                    resultPoint = queryResults[i]
                    if clusterResult[resultPoint] == UNCLASSIFIED:
                        seeds.append(resultPoint)
                        clusterResult[resultPoint] = clusterId
                    elif clusterResult[resultPoint] == NOISE:
                        clusterResult[resultPoint] = clusterId
            seeds = seeds[1:]
        return True

def dbscan(data, eps, minPts):
    """
    输入：数据集, 半径大小, 最小点个数
    输出：分类簇id
    """
    clusterId = 1
    nPoints = data.shape[1]
    clusterResult = [UNCLASSIFIED] * nPoints
    for pointId in range(nPoints):
        point = data[:, pointId]
        if clusterResult[pointId] == UNCLASSIFIED:
            if expand_cluster(data, clusterResult, pointId, clusterId, eps, minPts):
                clusterId = clusterId + 1
    return clusterResult, clusterId - 1

def plotFeature(data, clusters, clusterNum):
    nPoints = data.shape[1]
    matClusters = np.mat(clusters).transpose()
    fig = plt.figure()
    scatterColors = ['black', 'blue', 'green', 'yellow', 'red', 'purple', 'orange', 'brown']
    ax = fig.add_subplot(111)
    for i in range(clusterNum + 1):
        colorSytle = scatterColors[i % len(scatterColors)]
        subCluster = data[:, np.nonzero(matClusters[:, 0].A == i)]
        ax.scatter(subCluster[0, :].flatten().A[0], subCluster[1, :].flatten().A[0], c=colorSytle, s=50)

def main():
    dataSet = loadDataSet('788points.txt', splitChar=',')
    dataSet = np.mat(dataSet).transpose()
    # print(dataSet)
    clusters, clusterNum = dbscan(dataSet, 2, 15)
    print("cluster Numbers = ", clusterNum)
    # print(clusters)
    plotFeature(dataSet, clusters, clusterNum)

if __name__ == '__main__':
    start = time.clock()
    main()
    end = time.clock()
    print('finish all in %s' % str(end - start))
    plt.show()
```

#### 输出样例

```python
cluster Numbers =  7
finish all in 32.712135628590794
```

![img](http://images2015.cnblogs.com/blog/701997/201602/701997-20160203231531507-1809502270.jpg)

## 利用sklearn实现

#### 1. scikit-learn中的DBSCAN类

　　　　在scikit-learn中，DBSCAN算法类为sklearn.cluster.DBSCAN。要熟练的掌握用DBSCAN类来聚类，除了对DBSCAN本身的原理有较深的理解以外，还要对最近邻的思想有一定的理解。集合这两者，就可以玩转DBSCAN了。

#### 2. DBSCAN类重要参数

　　　　DBSCAN类的重要参数也分为两类，一类是DBSCAN算法本身的参数，一类是最近邻度量的参数，下面我们对这些参数做一个总结。

　　　　1）**eps**： DBSCAN算法参数，即我们的![img](https://mathjax.cnblogs.com/2_7_2/fonts/HTML-CSS/TeX/png/Math/Italic/283/03F5.png?V=2.7.2)ϵ-邻域的距离阈值，和样本距离超过![img](https://mathjax.cnblogs.com/2_7_2/fonts/HTML-CSS/TeX/png/Math/Italic/283/03F5.png?V=2.7.2)ϵ的样本点不在![img](https://mathjax.cnblogs.com/2_7_2/fonts/HTML-CSS/TeX/png/Math/Italic/283/03F5.png?V=2.7.2)ϵ-邻域内。默认值是0.5.一般需要通过在多组值里面选择一个合适的阈值。eps过大，则更多的点会落在核心对象的![img](https://mathjax.cnblogs.com/2_7_2/fonts/HTML-CSS/TeX/png/Math/Italic/283/03F5.png?V=2.7.2)ϵ-邻域，此时我们的类别数可能会减少， 本来不应该是一类的样本也会被划为一类。反之则类别数可能会增大，本来是一类的样本却被划分开。

　　　　2）**min_samples**： DBSCAN算法参数，即样本点要成为核心对象所需要的![img](https://mathjax.cnblogs.com/2_7_2/fonts/HTML-CSS/TeX/png/Math/Italic/283/03F5.png?V=2.7.2)ϵ-邻域的样本数阈值。默认值是5. 一般需要通过在多组值里面选择一个合适的阈值。通常和eps一起调参。在eps一定的情况下，min_samples过大，则核心对象会过少，此时簇内部分本来是一类的样本可能会被标为噪音点，类别数也会变多。反之min_samples过小的话，则会产生大量的核心对象，可能会导致类别数过少。

　　　　3）**metric**：最近邻距离度量参数。可以使用的距离度量较多，一般来说DBSCAN使用默认的欧式距离（即p=2的闵可夫斯基距离）就可以满足我们的需求。

　　　　4）**algorithm**：最近邻搜索算法参数，算法一共有三种，第一种是蛮力实现，第二种是KD树实现，第三种是球树实现。这三种方法在[K近邻法(KNN)原理小结](http://www.cnblogs.com/pinard/p/6061661.html)中都有讲述，如果不熟悉可以去复习下。对于这个参数，一共有4种可选输入，‘brute’对应第一种蛮力实现，‘kd_tree’对应第二种KD树实现，‘ball_tree’对应第三种的球树实现， ‘auto’则会在上面三种算法中做权衡，选择一个拟合最好的最优算法。需要注意的是，如果输入样本特征是稀疏的时候，无论我们选择哪种算法，最后scikit-learn都会去用蛮力实现‘brute’。个人的经验，一般情况使用默认的 ‘auto’就够了。 如果数据量很大或者特征也很多，用"auto"建树时间可能会很长，效率不高，建议选择KD树实现‘kd_tree’，此时如果发现‘kd_tree’速度比较慢或者已经知道样本分布不是很均匀时，可以尝试用‘ball_tree’。而如果输入样本是稀疏的，无论你选择哪个算法最后实际运行的都是‘brute’。

　　　　5）**leaf_size**：最近邻搜索算法参数，为使用KD树或者球树时， 停止建子树的叶子节点数量的阈值。这个值越小，则生成的KD树或者球树就越大，层数越深，建树时间越长，反之，则生成的KD树或者球树会小，层数较浅，建树时间较短。默认是30. 因为这个值一般只影响算法的运行速度和使用内存大小，因此一般情况下可以不管它。

　　　　6） **p**: 最近邻距离度量参数。只用于闵可夫斯基距离和带权重闵可夫斯基距离中p值的选择，p=1为曼哈顿距离， p=2为欧式距离。如果使用默认的欧式距离不需要管这个参数。

　　　　以上就是DBSCAN类的主要参数介绍，其实需要调参的就是两个参数eps和min_samples，这两个值的组合对最终的聚类效果有很大的影响。

#### 3. scikit-learn DBSCAN聚类实例

　　　　首先，我们生成一组随机数据，为了体现DBSCAN在非凸数据的聚类优点，我们生成了三簇数据，两组是非凸的。代码如下：

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
%matplotlib inline
X1, y1=datasets.make_circles(n_samples=5000, factor=.6,
                                      noise=.05)
X2, y2 = datasets.make_blobs(n_samples=1000, n_features=2, centers=[[1.2,1.2]], cluster_std=[[.1]],
               random_state=9)

X = np.concatenate((X1, X2))
plt.scatter(X[:, 0], X[:, 1], marker='o')
plt.show()
```

　　　　可以直观看看我们的样本数据分布输出：

![img](http://images2015.cnblogs.com/blog/1042406/201612/1042406-20161224183433417-1000123917.png)

　　　　首先我们看看K-Means的聚类效果，代码如下：

```python
from sklearn.cluster import KMeans
y_pred = KMeans(n_clusters=3, random_state=9).fit_predict(X)
plt.scatter(X[:, 0], X[:, 1], c=y_pred)
plt.show()
```

　　　　K-Means对于非凸数据集的聚类表现不好，从上面代码输出的聚类效果图可以明显看出，输出图如下：

![img](http://images2015.cnblogs.com/blog/1042406/201612/1042406-20161224183823073-116317869.png)

　　　　那么如果使用DBSCAN效果如何呢？我们先不调参，直接用默认参数，看看聚类效果,代码如下：

```python
from sklearn.cluster import DBSCAN
y_pred = DBSCAN().fit_predict(X)
plt.scatter(X[:, 0], X[:, 1], c=y_pred)
plt.show()
```

　　　　发现输出让我们很不满意，DBSCAN居然认为所有的数据都是一类！输出效果图如下：

![img](http://images2015.cnblogs.com/blog/1042406/201612/1042406-20161224184226964-988053285.png)

　　　　怎么办？看来我们需要对DBSCAN的两个关键的参数eps和min_samples进行调参！从上图我们可以发现，类别数太少，我们需要增加类别数，那么我们可以减少![img](https://mathjax.cnblogs.com/2_7_2/fonts/HTML-CSS/TeX/png/Math/Italic/283/03F5.png?V=2.7.2)ϵ-邻域的大小，默认是0.5，我们减到0.1看看效果。代码如下：

```python
y_pred = DBSCAN(eps = 0.1).fit_predict(X)
plt.scatter(X[:, 0], X[:, 1], c=y_pred)
plt.show()
```

　　　　对应的聚类效果图如下：

 ![img](http://images2015.cnblogs.com/blog/1042406/201612/1042406-20161224184705651-1252197402.png)

　　　　可以看到聚类效果有了改进，至少边上的那个簇已经被发现出来了。此时我们需要继续调参增加类别，有两个方向都是可以的，一个是继续减少eps，另一个是增加min_samples。我们现在将min_samples从默认的5增加到10，代码如下：

```python
y_pred = DBSCAN(eps = 0.1, min_samples = 10).fit_predict(X)
plt.scatter(X[:, 0], X[:, 1], c=y_pred)
plt.show()
```

　　　　输出的效果图如下：

![img](http://images2015.cnblogs.com/blog/1042406/201612/1042406-20161224184944698-2075899409.png)

 

　　　　可见现在聚类效果基本已经可以让我们满意了。

　　　　上面这个例子只是帮大家理解DBSCAN调参的一个基本思路，在实际运用中可能要考虑很多问题，以及更多的参数组合，希望这个例子可以给大家一些启发。