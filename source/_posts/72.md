---
title: 科普一下AlphaGo的论文算法
date: 2017-11-26 13:52:48
tags: 科普
categories: 科普
---

二十年前我还是一名本科生的时候，就对计算机算法很感兴趣。当时深蓝战胜了卡斯帕罗夫，大家都普遍会议论到围棋，并且基本的观点都一致，就是计算机虽然在国际象棋上战胜了人类，但是离在围棋上战胜人类还有相当遥远的距离。没想到二十年后，我已经可以借助先进的4G通讯技术，实时收看AlphaGo在围棋上击败人类的全过程，真的是感慨万千。

虽然我不做科研很多年，但出于兴趣还是将DeepMind团队发表在Nature上的论文阅读了一遍。之后发现，很多围棋爱好者、很多对AI感兴趣的人虽然在网上发表了诸多议论，但是很少有真正了解AlphaGo是怎样“思考”和下棋的。考虑到很多AI领域、深度学习领域的专家不屑于科普AlphaGo的“算法”，而更多的人又不愿意去啃那篇论文，干脆我就来抛砖引玉，将AlphaGo的“思考过程”和大家做个普及性分享，并谈谈自己针对未来AI和深度学习领域的认识。

**一、AlphaGo“思考”的过程**

考虑到我们人类认识问题都愿意自顶向下，先看到全局再看局部。所以我先介绍一下AlphaGo“思考”的全过程。

形象地说，AlphaGo有四个思考用的“大脑”，也就是DeepMind团队训练出来的四个神经网络，用论文中的符号表示，就是Pπ、Pσ、Pρ和Vθ，为了方便起见，给它们起名为**“快速走子网络”**、**“专家训练网络”**、**“自我提升网络”**和**“价值判断网络”**。前三个神经网络都以当前围棋对弈局面为输入，经过计算后输出可能的走子选择和对应的概率，概率越大的点意味着神经网络更倾向于在那一点走子，这个概率是针对输入局面下所有可能的走子方法而计算的，也就是每个可能的落子点都有一个概率，当然会有不少的点概率为0。第四个神经网络是进行价值判断的，输入一个对弈局面，它会计算得出这个局面下黑棋和白棋的胜率。

简单的解释一下前三个网络的区别：**“快速走子网络”**是一个比较低水平但是计算量也很小的神经网络；**“专家训练网络”**的参数都是通过职业棋手对弈的棋局训练出来的，它的激活函数和具体的卷积核数量以及相应神经元数量会与“快速走子网络”有所不同，表现为计算量不同，水平也不同；**“自我提升网络”**是在“专家训练网络”的基础上，通过电脑自我对弈的大量棋局进行提升训练后的网络，理论上讲水平更高，计算量与“专家训练网络”是一样的，只是训练出来的参数不同。

训练好这四个神经网络之后，AlphaGo就可以开始与人对弈了。对弈过程中，AlphaGo的“思考”是通过蒙特卡洛博弈树搜索和模拟来实现的。大致步骤如下：

（1）假设当前棋局状态为St，对于每一种可选择的走法a，选择走a之后的棋局价值Q(St,a)与“专家训练网络”计算出的走a的概率P(St,a)之和最大的那种a，记为at。**注意，这里面的Q(St,a)不是简单的靠“价值判断网络”计算出来的，而是“价值判断网络”计算结果与蒙特卡洛模拟结果的加权平均；这里的P(St,a)也不是直接用“专家训练网络”计算出来的，而是正比于Pσ(St,a)／（1+N(St,a)），N(St,a)是（St，a）这个节点所经过的搜索次数，为了鼓励搜索模拟，“专家训练网络”所得到的走子概率用搜索次数进行了衰减。**

（2）按照（1）中的方法继续搜索选择下一级节点，直到搜索下去碰上一个叶子节点，也就是原来没有再继续展开的、没有评估过的节点。

（3）将这个叶子节点SL展开，并用“价值判断网络”计算其价值Vθ（SL），然后用“快速走子网络”在这个节点的基础上进行多局自我对弈，根据多局对弈的胜负比率来估算胜率Z(SL)。最后使用Vθ（SL）和Z(SL)的加权平均来估算此节点的胜率。

（4）将估算结果反向更新到这次搜索途经的全部节点，反向更新公式稍复杂，就不再列了，本来目的就是普及性介绍嘛。

（5）之后再从St开始，仍然按照（1）的规则重新搜索。

至于蒙卡搜索模拟到什么时候，取决于给AlphaGo多长的时间走一步棋，时间快到的时候，AlphaGo就停止搜索模拟，并以跟节点St下搜索途经次数最多的节点（因为每次都是选最佳节点搜索模拟，所以搜索结束后就以途经次数最多作为标准了）作为自己本步的着法。

**以上就是AlphaGo思考的全过程，其实和人类很类似，有思考下一步着法的“大脑”，有判断局面价值的“大脑”，然后再向后推断若干步，确定自己的“走法”。**

**二、卷积神经网络（CNN）的极简介绍**

下面简单介绍一下卷积神经网络。先说神经网络，就是模拟人类或者动物大脑，用若干个神经元共同计算逼近某种复杂计算（函数）的方法。其实任何一种价值判断都可以理解为某种多元函数，输入若干数据（信息），输出结论。数学上可以证明，使用神经网络（多层）可以无限逼近这些多元函数。

拿围棋来讲，假设每种局面下会有一种或几种最理想的走法，那么就可以将局面作为输入，理想走法作为输出形成一类多元函数。理论上，神经网络可以无限逼近这个函数。由于围棋局面可以看成一个19＊19的图像，而卷积神经网络（CNN）又是处理图像比较理想的方法，所以DeepMind团队就使用了CNN。

当然CNN为什么设置为13层的神经网络，每层的卷积核有几个，激活函数是什么，使用什么样的误差传递函数来反向训练这个神经网络，这些都需要尝试，这才是DeepMind团队最主要的成果，当然论文里面也不会详细说了。

一个问题是，训练最基础的“专家训练网络”所使用的数据是大量职业棋手的棋局，但是没有理由认为职业棋手的走法就是最佳走法，所以这种训练实际上是用一种有误差的数据进行的，当然训练出来的神经网络也不会绝对理想。但是，如果AlphaGo真的在这种训练下达到高水平，以后可以考虑使用高水平AlphaGo自我对弈的棋局重新训练形成“专家训练网络”，也许效果会更好。

**三、关于论文中的几个有趣事实**

（1）“快速走子网络”计算一次需要2微秒，“专家训练网络”计算一次需要3毫秒。

（2）“快速走子网络”与专家走法的匹配准确度为24.2%，“专家训练网络”则为57%。

（3）“自我提升网络”和“专家训练网络”对弈胜率为80%。

（4）“价值判断网络”在使用职业棋手对局数据进行训练时，发生了过度拟合的情况，训练组偏差0.19而测试组达到0.37，说明泛化效果不好。为了解决这个问题，改用了“自我提升网络”自我对弈3000万局作为“价值判断网络”的训练数据，基本解决了过度拟合的问题。

（5）DeepMind团队发现，在蒙特卡洛树搜索时，计算下一步走子概率使用“专家训练网络”效果要优于使用“自我提升网络”，虽然“自我提升网络”与“专家训练网络”对弈时胜率高达80%。但是在训练“价值判断网络”时，使用“自我提升网络”自我对弈的棋局效果好于使用“专家训练网络”。

（6）计算上，多线程搜索使用CPU处理，策略和价值并行计算使用GPU处理。单机版的AlphaGo使用了40个线程、48个CPU和8个GPU。分布式版的AlphaGo使用了40个线程、1202个CPU和176个GPU。

（7）分布式版本对单机版的胜率为77%。

**四、澄清一些观点及个人思考**

（1）AlphaGo有自己的“棋风”么？

从人类的角度看，某个固定版本的AlphaGo肯定会有自己的“棋风”，因为训练好的神经网络参数就决定了它会如何“判断”，蒙卡搜索算法又决定了它的“思考”过程，这些综合在一起就形成了它的走棋风格。但是这种风格是在大量数据训练后形成的，肯定与人类的风格很不一样。它未必有系统的、前后一致的特点，它的风格更多体现在某种局面下会有怎样的判断倾向。当然，是否能够被人类准确抓住不好说。

（2）AlphaGo在第四局“抽风”的bug好解决么？

个人认为不好解决。训练形成的神经网络里面有大量的参数，这些参数都不是程序员设定的，而是软件自己学习形成的。如果这些参数不尽合理，在某些局面下会误判，那么可能的解决办法就是重新训练或者加强训练，绝不可能由哪个人直接修改某些参数来消除bug。严格的说，没有哪个人敢随意修改神经网络参数，所以在与李世石这五局对局过程中，AlphaGo版本是没有任何变化的。

（3）关于人工智能在深度学习技术下的可能发展？

首先，个人认为那种能够威胁人类的AI还远远看不到希望，目前的AlphaGo是在“监督”下学习，还算不上完全自我学习。即使不久的将来能自我学习了，也不过是针对围棋，并不是万能的“学者”。我觉得人类还没必要担心AI会威胁人类。

其次，AI这次在围棋上战胜人类顶尖高手，基本证明了所谓的“棋感”、“棋风”、“大局观”等围棋高手所谈论的虚的能力，并不是人类独有的，经过训练的神经网络也会有。所以，随着技术的进步，我相信电脑也会能够欣赏艺术（音乐、画作、小说、笑话），能够创作文学、艺术作品，能够针对不同的情况形成自己的“情绪”。但是这些都不是人类害怕AI的理由，因为这些始终都是通过计算实现的，其实是我们人类可控的。

未来的AI可以帮助人类搞科研、分析数据、协助医疗、创作诗歌、写新闻报道等等，我相信这些都会是人类科技的进步，都会让生活更美好。当然在享受这些美好的同时，也需要记住，世界上所有的事情都是双刃剑，AI也可以用来骗人、作恶，这就需要善良的人类通过有效的管理措施和监督措施，通过法律，禁止人们开展“坏”AI的研究。

文/赵昊彤