---
title: BP神经网络算法
date: 2016-07-29 13:13:06
tags: 神经网络
---

> 　　BP（Back Propagation）网络是1986年由Rumelhart和McCelland为首的科学家小组提出，是一种按误差逆传播算法训练的多层前馈网络，是目前应用最广泛的神经网络模型之一。BP网络能学习和存贮大量的输入-输出模式映射关系，而无需事前揭示描述这种映射关系的数学方程。它的学习规则是使用最速下降法，通过反向传播来不断调整网络的权值和阈值，使网络的误差平方和最小。BP神经网络模型拓扑结构包括输入层（input）、隐层(hide layer)和输出层(output layer)
>

　　人工神经网络（ANN）系统是２０世纪４０年代后出现的，它是由众多的神经元可调的连接权值连接而成，具有大规模并行处理、分布式信息存储、良好的自组织自学习能力等特点，在信息处理、模式识别、智能控制及系统建模等领域得到越来越广泛的应用。尤其误差反向传播算法（BP网络）可以逼近任意连续函数，具有很强的非线性映射能力，而且网络的中间层数、各层的处理单元数及网络的学习系数等参数可根据具体情况设定，灵活性很大，所以它在许多应用领域中起到重要作用。近年来，为了解决BP神经网络收敛速度慢、不能保证收敛到全局最小点，网络的中间层及它的单元数选取无理论指导及网络学习和记忆的不稳定性等缺陷，提出了许多改进算法。
<!--more-->
　　１ 传统的BP算法简述

　　BP算法是一种有监督式的学习算法，其主要思想是：输入学习样本，使用反向传播算法对网络的权值和偏差进行反复的调整训练，使输出的向量与期望向量尽可能地接近，当网络输出层的误差平方和小于指定的误差时训练完成，保存网络的权值和偏差。具体步骤如下：

　　（１）初始化，随机给定各连接权［ｗ］,［ｖ］及阀值θi，ｒt。

　　（２）由给定的输入输出模式对计算隐层、输出层各单元输出

　　ｂj＝ｆ（■wijai-θj） ｃt＝ｆ（■vjtbj－ｒt）

　　式中：ｂj为隐层第ｊ个神经元实际输出；ｃt为输出层第ｔ个神经元的实际输出；wij为输入层至隐层的连接权；vjt为隐层至输出层的连接权。

　　ｄtk＝（ｙtk－ｃt）ｃt（１－ｃt） ｅjk＝［■dtvjt］ ｂj（１－ｂj）

　　式中：ｄtk为输出层的校正误差；ｅjk为隐层的校正误差。

　　（３）计算新的连接权及阀值，计算公式如下：

　　ｖjt（ｎ＋１）＝ｖjt（ｎ）＋?琢ｄtkｂj ｗij（ｎ＋１）＝ｗij（ｎ）＋?茁ｅjkａik

　　ｒt（ｎ＋１）＝ｒt（ｎ）＋?琢ｄtk θj（ｎ＋１）=θj（ｎ）＋?茁ｅjk

　　式中：?琢，?茁为学习系数（０＜?琢＜１，０＜?茁＜１）。

　　（４）选取下一个输入模式对返回第２步反复训练直到网络设输出误差达到要求结束训练。

　　传统的ＢＰ算法，实质上是把一组样本输入/输出问题转化为一个非线性优化问题，并通过负梯度下降算法，利用迭代运算求解权值问题的一种学习方法，但其收敛速度慢且容易陷入局部极小，为此提出了一种新的算法，即高斯消元法。

　　２ 改进的ＢＰ网络算法

　　２．１ 改进算法概述

　　此前有人提出：任意选定一组自由权，通过对传递函数建立线性方程组，解得待求权。本文在此基础上将给定的目标输出直接作为线性方程等式代数和来建立线性方程组，不再通过对传递函数求逆来计算神经元的净输出，简化了运算步骤。没有采用误差反馈原理，因此用此法训练出来的神经网络结果与传统算法是等效的。其基本思想是：由所给的输入、输出模式对通过作用于神经网络来建立线性方程组，运用高斯消元法解线性方程组来求得未知权值，而未采用传统ＢＰ网络的非线性函数误差反馈寻优的思想。

　　２．２ 改进算法的具体步骤

　　对给定的样本模式对，随机选定一组自由权，作为输出层和隐含层之间固定权值，通过传递函数计算隐层的实际输出，再将输出层与隐层间的权值作为待求量，直接将目标输出作为等式的右边建立方程组来求解。

　　现定义如下符号（见图１）：ｘ （p）输入层的输入矢量；ｙ （p）输入层输入为ｘ （p）时输出层的实际输出矢量；ｔ （p）目标输出矢量；ｎ，ｍ，ｒ分别为输入层、隐层和输出层神经元个数；Ｗ为隐层与输入层间的权矩阵；Ｖ为输出层与隐层间的权矩阵。具体步骤如下：

　　（１）随机给定隐层和输入层间神经元的初始权值ｗij。

　　（２）由给定的样本输入ｘi（p）计算出隐层的实际输出ａj（p）。为方便起见将图１网络中的阀值写入连接权中去，令：隐层阀值θj＝ｗnj，ｘ（ｎ）＝－１，则：

　　ａj（p）=ｆ（■wijｘi（p）） （ｊ＝１，２…ｍ－１）。

　　（３）计算输出层与隐层间的权值ｖjr。以输出层的第ｒ个神经元为对象，由给定的输出目标值ｔr（p）作为等式的多项式值建立方程，用线性方程组表示为：

　　a0（1）v1r+a1（1）v2r+…+am（1）vmr=tr（1）a0（2）v1r+a1（2）v2r+…+am（2）vmr=tr（2） ……a0（p）v1r+a1（p）v2r+…+am（p）vmr=tr（p） 简写为： Ａｖ＝Ｔ

　　为了使该方程组有唯一解，方程矩阵Ａ为非奇异矩阵，其秩等于其增广矩阵的秩，即：ｒ（Ａ）＝ｒ（Ａ┊Ｂ），且方程的个数等于未知数的个数，故取ｍ＝ｐ，此时方程组的唯一解为： Ｖｒ＝［ｖ0r，ｖ2r，…ｖmr］（ｒ＝０，１，２…ｍ－１） 

　　（４）重复第三步就可以求出输出层ｍ个神经元的权值，以求的输出层的权矩阵加上随机固定的隐层与输入层的权值就等于神经网络最后训练的权矩阵。

　　３ 计算机运算实例

　　现以神经网络最简单的ＸＯＲ问题用ＶＣ编程运算进行比较（取神经网络结构为２－４－１型），传统算法和改进ＢＰ算法的误差（取动量因子α＝０．００１ ５，步长η＝１．６５３）

 

算法C++版代码：

```c++
#ifndef BP_H

#define BP_H

class BpNet

{

private:

    int _nInput;            //输入层节点个数

    int _nHide;             //隐含层节点个数

    int _nOutput;           //输出层节点个数

    

    double **_pplfWeight1;  //输入层-隐含层权系数

    double **_pplfWeight2;  //隐含层-输出层权系数

    

    double *plfHideIn, *plfHideOut;       //隐含层的网络输入和输出

    double *plfOutputIn, *plfOutputOut;   //输出层的网络输入和输出

    

private:

    double (*f)(double);    //激活函数

    

public:

    

    BpNet(int nInput, int nHide, int nOutput);

    

    

    virtual ~BpNet();

    

    

    bool Train(int n, double pplfInput, double pplfDesire);

    

    

    void Classify(double plfInput[], double plfOutput[]);

};

#endif

 

#include "bp.h"

#include "memory.h"

#include "math.h"

#include "time.h"

#include "stdlib.h"

#include "stdio.h"

double sigmoid(double x)

{

    return 1 / (1 + exp(- x));

}

BpNet::BpNet(int nInput, int nHide, int nOutput)

{

    _nInput = nInput;

    _nHide = nHide;

    _nOutput = nOutput;

    

    srand((unsigned)time(NULL));

    

    pplfWeight1 = new double *[nInput];

    for (int i = 0; i < _nInput; i++)

    {

        pplfWeight1[i] = new double [nHide];

        for (int j = 0; j < _nHide; j++) _pplfWeight1i = rand() / (double)(RAND_MAX);

    }

    

    pplfWeight2 = new double *[nHide];

    for (int i = 0; i < _nHide; i++)

    {

        pplfWeight2[i] = new double [nOutput];

        for (int j = 0; j < _nOutput; j++) _pplfWeight2i = rand() / (double)(RAND_MAX);

    }

    

    plfHideIn = new double[nHide];

    plfHideOut = new double[nHide];

    plfOutputIn = new double[nOutput];

    plfOutputOut = new double[nOutput];

    f = sigmoid;    //使用sigmoid激活函数

}

BpNet::~BpNet()

{

    delete []_plfHideIn;

    delete []_plfHideOut;

    delete []_plfOutputIn;

    delete []_plfOutputOut;

    for (int i = 0; i < nInput; i++) delete []pplfWeight1[i];

    for (int i = 0; i < nHide; i++) delete []pplfWeight2[i];

    delete []_pplfWeight1;

    delete []_pplfWeight2;

}

void BpNet::Classify(double plfInput[], double plfOutput[])

{

    memset(_plfHideIn, 0, sizeof(double) * _nHide);

    memset(_plfHideOut, 0, sizeof(double) * _nHide);

    memset(_plfOutputIn, 0, sizeof(double) * _nOutput);

    memset(_plfOutputOut, 0, sizeof(double) * _nOutput);

    

    //输入层到隐含层的正向传播

    for (int i = 0; i < _nInput; i++)

        for (int j = 0; j < _nHide; j++)

            _plfHideIn[j] += plfInput[i] * _pplfWeight1i;

    for (int j = 0; j < nHide; j++) _plfHideOut[j] = (*f)(plfHideIn[j]);

    

    //隐含层到输出层的正向传播

    for (int j = 0; j < _nHide; j++)

        for (int k = 0; k < _nOutput; k++)

            _plfOutputIn[k] += _plfHideOut[j] * _pplfWeight2j;

    for (int k = 0; k < nOutput; k++) _plfOutputOut[k] = (*f)(plfOutputIn[k]);

    

    if (plfOutput != NULL)

        memcpy(plfOutput, _plfOutputOut, sizeof(double) * _nOutput);

}

bool BpNet::Train(int n, double pplfInput, double pplfDesire)

{

    const double a = 0.1;

    const double E = 0.01;

    double lfE = E + 1;

    

    //输入层-隐含层权系数增量

    double **pplfDeltaWeight1 = new double *[_nInput];

    for (int i = 0; i < _nInput; i++)

    {

        pplfDeltaWeight1[i] = new double [_nHide];

        memset(pplfDeltaWeight1[i], 0, sizeof(double) * _nHide);

    }

    

    //隐含层-输出层权系数增量

    double **pplfDeltaWeight2 = new double *[_nHide];

    for (int i = 0; i < _nHide; i++)

    {

        pplfDeltaWeight2[i] = new double[_nOutput];

        memset(pplfDeltaWeight2[i], 0, sizeof(double) * _nOutput);

    }

        

    int nCount = 0;

    while (lfE > E)

    {

        lfE = 0;

        //对每一个样本进行处理

        for (int i = 0; i < n; i++)

        {

            double *plfInput = pplfInput[i];        //样本输入

            double *plfDesire = pplfDesire[i];      //样本期望输出

        

            //计算样本实际输出plfOutput

            Classify(plfInput, NULL);

        

            //计算误差测度

            double lfEp = 0;

            for (int j = 0; j < _nOutput; j++)

                lfEp += (plfDesire[j] - _plfOutputOut[j]) * (plfDesire[j] - _plfOutputOut[j]);

            lfE += lfEp;

        

            //计算隐含层-输出层权系数增量

            double *plfChange2 = new double[_nOutput];

        

            for (int j = 0; j < _nOutput; j++)

                plfChange2[j] = _plfOutputOut[j] * (1 - _plfOutputOut[j]) * (plfDesire[j] - _plfOutputOut[j]);

            for (int j = 0; j < _nHide; j++)

                for (int k = 0; k < _nOutput; k++)

                    pplfDeltaWeight2j = a * _plfHideOut[j] * plfChange2[k];

            //计算输入层-隐含层权系数增量

            double *plfChange1 = new double[_nHide];

            memset(plfChange1, 0, sizeof(double) * _nHide);

            for (int j = 0; j < _nHide; j++)

            {

                for (int k = 0; k < _nOutput; k++)

                    plfChange1[j] += _pplfWeight2j * plfChange2[k];

                plfChange1[j] *= _plfHideOut[j] * (1 - _plfHideOut[j]);

            }

            for (int j = 0; j < _nInput; j++)

                for (int k = 0; k < _nHide; k++)

                    pplfDeltaWeight1j = a * plfInput[j] * plfChange1[k];

                

            delete []plfChange1;

            delete []plfChange2;

        

            //更新Bp网络权值

            for (int i = 0; i < _nInput; i++)

                for (int j = 0; j < _nHide; j++)

                    _pplfWeight1i += pplfDeltaWeight1i;

            for (int i = 0; i < _nHide; i++)

                for (int j = 0; j < _nOutput; j++)

                    _pplfWeight2i += pplfDeltaWeight2i;

        }

        nCount++;

        if (nCount % 10 == 0) printf("%d%8.5lf\n", nCount, lfE);

    }

            

    for (int i = 0; i < _nInput; i++) delete []pplfDeltaWeight1[i];

    for (int i = 0; i < _nHide; i++) delete []pplfDeltaWeight2[i];

    delete [] pplfDeltaWeight1;

    delete [] pplfDeltaWeight2;

    return true;

}

```